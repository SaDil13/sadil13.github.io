---
permalink: /
title: "Ziming Wei(魏梓铭)@SYSU"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<br/>

# About Me

I'm currently a second-year master's student at [Sun Yat-sen University (SYSU)](https://www.sysu.edu.cn/), fortunately advised by Prof. [Xiaodan Liang](https://scholar.google.com/citations?user=voxznZAAAAAJ&hl) and Dr. [Bingqian Lin](https://scholar.google.com/citations?user=7tNbAJcAAAAJ) at the [Human Cyber Physical Intelligence Integration Lab (HCP-I2 Lab)](https://www.sysu-hcp.net/). Previously, I obtained my bachelor's degree of Intelligence Science and Technology from [Sun Yat-sen University](https://www.sysu.edu.cn/) in 2024.

My research interest lies in Multi-modal understanding, learning and data generation, Embodied AI, Spatial Intelligence, and Embodied generalist agents.

<br/>
<br/>

# Education

- 09/2024 ~ 06/2027(Expected): [School of Intelligent Systems Engineering](https://ise.sysu.edu.cn/ "APMA, Brown"){:target="_blank"}, [Sun Yat-sen University](https://www.sysu.edu.cn/ "Brown"){:target="_blank"}
  - M.Eng. in *Control Science and Engineering* (Advisor: Prof. [Xiaodan Liang](https://scholar.google.com/citations?user=voxznZAAAAAJ&hl), Weighted Average Score: 91.74/100.0)
- 09/2020 ~ 06/2024: [School of Intelligent Systems Engineering](https://ise.sysu.edu.cn/ "SCGY, USTC"){:target="_blank"}, [Sun Yat-sen University](https://www.sysu.edu.cn/ "USTC"){:target="_blank"}
  - B.Sc. in *Intelligent Science and Technology* (GPA: 4.03/5.0, Ranking: 3/226, Top 1%)

<br/>
<br/>

# Publications and Preprints

<div class="publication row clearfix">
    <div class="row-text">
        <a class="publication-title bold" href="https://arxiv.org/abs/2505.20148">MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents</a><br/>
        <span class="bold"><b>Ziming Wei</b></span><sup title="Equal Contribution">*</sup>, Bingqian Lin<sup title="Equal Contribution">*</sup>, Zijian Jiao<sup title="Equal Contribution">*</sup>, Yunshuang Nie, Liang Ma, Yuecheng Liu, Yuzheng Zhuang, Xiaodan Liang<sup title="Corresponding Author">&dagger;</sup><br/>
        (<span class="italic">NeurIPS 2025 Datasets and Benchmarks Track</span>)<br/>
        <!-- (<span class="italic">Under Review</span>)<br/> -->
        <a class="btn btn-dark" href="https://mineanybuild.github.io/">project</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2505.20148">arXiv</a> / <a class="btn" href="https://github.com/MineAnyBuild/MineAnyBuild">code</a> / <a class="btn btn-dark" href="https://huggingface.co/datasets/SaDil/MineAnyBuild">dataset</a> / <a class="btn" href="/assets/bibtex/mineanybuild.bib">bibtex</a> 
    </div>
</div>
<br/>
<div class="publication row clearfix">
    <div class="row-text">
        <a class="publication-title bold" href="https://arxiv.org/abs/2503.18065">Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation</a><br/>
        <span class="bold"><b>Ziming Wei</b></span><sup title="Equal Contribution">*</sup>, Bingqian Lin<sup title="Equal Contribution">*</sup>, Yunshuang Nie, Jiaqi Chen, Shikui Ma, Hang Xu, Xiaodan Liang<sup title="Corresponding Author">&dagger;</sup><br/>
        <!-- <span class="italic">TNNLS</span>, 2025<br/> -->
        (<span class="italic">Under Review</span>)<br/>
        <a class="btn btn-red" href="https://arxiv.org/abs/2503.18065">arXiv</a> / <a class="btn" href="https://github.com/SaDil13/VLN-RAM">code</a> / <a class="btn" href="/assets/bibtex/vlnram.bib">bibtex</a> 
    </div>
</div>
<br/>
<div class="publication row clearfix">
    <div class="row-text">
        <a class="publication-title bold" href="https://arxiv.org/abs/2403.07376">NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</a><br/>
        <span class="bold">Bingqian Lin</span><sup title="Equal Contribution">*</sup>, Yunshuang Nie<sup title="Equal Contribution">*</sup>, <b>Ziming Wei</b>, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang<sup title="Corresponding Author">&dagger;</sup><br/>
        (<span class="italic">TPAMI</span>, 2025)<br/>
        <a class="btn btn-red" href="https://ieeexplore.ieee.org/document/10938647">Paper</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2403.07376">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/NavCoT">code</a> / <a class="btn btn-dark" href="/assets/bibtex/navcot.bib">bibtex</a>
    </div>
</div>
<br/>
<div class="publication row clearfix">
    <div class="row-text">
        <a class="publication-title bold" href="https://arxiv.org/abs/2405.18721">Correctable Landmark Discovery via Large Models for Vision-Language Navigation</a><br/>
        <span class="bold">Bingqian Lin</span><sup title="Equal Contribution">*</sup>, Yunshuang Nie<sup title="Equal Contribution">*</sup>, <b>Ziming Wei</b>, Yi Zhu, Hang Xu, Shikui Ma, Jianzhuang Liu, Xiaodan Liang<sup title="Corresponding Author">&dagger;</sup><br/>
        (<span class="italic">TPAMI</span>, 2024)<br/>
        <a class="btn btn-red" href="https://ieeexplore.ieee.org/document/10543121">Paper</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2405.18721">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/CONSOLE">code</a> / <a class="btn" href="/assets/bibtex/console.bib">bibtex</a> 
    </div>
</div>
<br/>
<div class="publication row clearfix">
    <div class="row-text">
        <a class="publication-title bold" href="https://arxiv.org/abs/2506.01551">EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation</a><br/>
        <span class="bold">Bingqian Lin</span><sup title="Equal Contribution">*</sup>, Yunshuang Nie<sup title="Equal Contribution">*</sup>, Khun Loun Zai, <b>Ziming Wei</b>, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han, Liang Lin, Cewu Lu, Xiaodan Liang<sup title="Corresponding Author">&dagger;</sup><br/>
        (<span class="italic">Under Review</span>)<br/>
        <a class="btn btn-red" href="https://arxiv.org/abs/2506.01551">arXiv</a> / <a class="btn" href="https://github.com/expectorlin/EvolveNav">code</a> / <a class="btn" href="/assets/bibtex/evolvenav.bib">bibtex</a> 
    </div>
</div>
<br/>
<div class="publication row clearfix">
    <div class="row-text">
        <a class="publication-title bold" href="https://arxiv.org/abs/2405.18721">PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly</a><br/>
        <span class="bold">Liang Ma</span><sup title="Equal Contribution">*</sup>, Jiajun Wen<sup title="Equal Contribution">*</sup>, Min Lin<sup title="Equal Contribution">*</sup>, Rongtao Xu<sup title="Equal Contribution">*</sup>, Xiwen Liang<sup title="Equal Contribution">*</sup>, Bingqian Lin, Jun Ma, Yongxin Wang, <b>Ziming Wei</b>, Haokun Lin, Mingfei Han, Meng Cao, Bokui Chen, Ivan Laptev, Xiaodan Liang<sup title="Corresponding Author">&dagger;</sup><br/>
        (<span class="italic">NeurIPS 2025 Datasets and Benchmarks Track</span>)<br/>
        <a class="btn btn-dark" href="https://phyblock.github.io/">project</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2506.08708">arXiv</a> / <a class="btn" href="https://github.com/PhyBlock/PhyBlock">code</a> / <a class="btn btn-dark" href="https://huggingface.co/datasets/PhyBlock/PhyBlock_Benchmark">dataset</a> / <a class="btn" href="/assets/bibtex/phyblock.bib">bibtex</a> 
    </div>
</div>

<br/>
<br/>

# Experience

- Research Intern, Huawei Noah's Ark Lab(诺亚方舟实验室), 2025.04-Present
- Data Engineering Intern, Tencent SSV ForGood Labs(向善实验室群), 2023.07-2023.11

<br/>
<br/>

# Honors and Awards

- First Class Award Scholarship of the Graduate School, Sun Yat-sen University, in 2024, 2025
- Excellent Bachelar Dissertation Award of Sun Yat-sen University, in 2024 ([Top 1 Recommended](https://ise.sysu.edu.cn/article/992))
- Third Class Award Scholarship of Sun Yat-sen University Excellent Student, Sun Yat-sen University, China, in 2020-2021, 2021-2022 and 2022-2023


<br/>
<div style="display: none;">
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=VkzRYatEIuQzDBjdwiH5ffJYC2q-lSTR3fZ20m9y4oc&cl=ffffff&w=a"></script>
</div>

